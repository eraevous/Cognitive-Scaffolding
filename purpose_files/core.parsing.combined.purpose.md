# Module: core.parsing
> Extract, chunk, and normalize document content for LLM-safe ingestion across file formats and workflows.

### 游꿢 Intent & Responsibility
- Extract raw text from various file formats (.txt, .md, .pdf, .docx) for processing.
- Identify latent topic boundaries via embedding-based clustering before chunking.
- Produce semantic-aware text chunks and normalize filenames for safe downstream use.

### 游닌 Inputs & 游닋 Outputs
| Direction | Name         | Type            | Brief Description                                                                 |
|-----------|--------------|-----------------|------------------------------------------------------------------------------------|
| 游닌 In     | filepath      | str             | Path to local document file                                                        |
| 游닌 In     | text          | str             | Raw extracted text from file (for chunking)                                       |
| 游닌 In     | name          | str             | Arbitrary string label to normalize                                               |
| 游닋 Out    | raw_text      | str             | Cleaned full text extracted from file                                             |
| 游닋 Out    | chunks        | List[str]       | Paragraph-preserving text chunks (within character limits)                        |
| 游닋 Out    | normalized    | str             | Identifier-safe version of input string                                           |

### 游댕 Dependencies
- `fitz` (PyMuPDF), `python-docx`, `markdown`, `os`
- `openai`, `numpy`, `scikit-learn` for embedding-based segmentation

### 丘뙖잺 AI-Memory Tags
- `@ai-assumes:` Input documents are well-formed and parsable by library of choice.
- `@ai-breakage:` PDF/Word libraries may fail silently if dependencies are missing or input is malformed.
- `@ai-risks:` Chunking by paragraph could overflow token limits if paragraphs are excessively long.

### 游딖 Dialogic Notes
- The parser uses different strategies per file extension, failing clearly if unsupported.
- Chunking preserves structure when possible; long paragraphs still split arbitrarily.
- Normalization is simple and irreversible (loss of case/formatting); designed for ID safety.
- Future extensions could add support for HTML/EPUB and fallback token-based chunking for edge cases.
- New `semantic_chunk_text` function performs window embedding, clustering, and boundary detection to create topic-coherent segments.
