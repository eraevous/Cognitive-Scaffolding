- @ai-path: core.parsing.semantic_chunk
- @ai-source-file: semantic_chunk.py
- @ai-role: analysis.utility
- @ai-intent: "Segment text using embedding-based window clustering to infer topic boundaries."
- @ai-version: 0.1.0
- @ai-generated: true
- @ai-verified: false
- @human-reviewed: false
- @schema-version: 0.2
- @ai-risk-pii: low
- @ai-risk-performance: "Relies on embedding model and clustering; heavy docs may incur cost."

# Module: core.parsing.semantic_chunk
> Produce topic-aware text chunks by embedding overlapping windows and clustering them.

### ðŸŽ¯ Intent & Responsibility
- Sample small windows from text (e.g., 200 tokens sliding by 100).
- Embed each window with OpenAI and cluster vectors via KMeans.
- Detect boundaries where cluster assignment changes to yield semantic segments.

### ðŸ“¥ Inputs & ðŸ“¤ Outputs
| Direction | Name | Type | Brief Description |
|-----------|------|------|-------------------|
| ðŸ“¥ In | text | str | Raw document text to segment |
| ðŸ“¥ In | window_tokens | int | Size of sliding windows for sampling |
| ðŸ“¥ In | step_tokens | int | Step between window starts |
| ðŸ“¥ In | n_clusters | int | Number of clusters for KMeans |
| ðŸ“¤ Out | chunks | List[str] | Topic-coherent text segments |

### ðŸ”— Dependencies
- `tiktoken` for tokenization
- `sklearn.cluster.KMeans`
- `core.embeddings.embedder.embed_text`

### ðŸ—£ Dialogic Notes
- Works best when text length >> window size, giving enough context for clustering.
- Future improvements might use HDBSCAN or heuristics to auto-select cluster count.

### 9â€…Pipeline Integration
- @ai-pipeline-order: inverse
- **Coordination Mechanics:** Called by `core.embeddings.embedder` when `segment_mode=True`; outputs are written to disk and embedded for vector search.
- **Integration Points:** Results feed into `core.retrieval.retriever`, `core.synthesizer` workflows, and TokenMap Analyzer.
- **Risks:** Overclustering can fragment context, while excessive window count drives GPU and API cost.
