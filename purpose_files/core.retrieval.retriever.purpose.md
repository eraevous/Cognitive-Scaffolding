- @ai-path: core.retrieval.retriever
- @ai-source-file: retriever.py
- @ai-role: logic
- @ai-intent: "Embed text queries and return top-k document filenames from the vector store."
- @ai-version: 0.1.0
- @ai-generated: true
- @ai-verified: false
- @human-reviewed: false
- @schema-version: 0.2
- @ai-risk-pii: low
- @ai-risk-performance: "Embedding model calls incur latency and cost."

# Module: core.retrieval.retriever
> Unified interface for semantic search against the FAISS vector store.

### 游꿢 Intent & Responsibility
- Convert query text to embeddings using the configured model.
 - Delegate search to `FaissStore` and return ranked document filenames using `id_map.json`.
- Provide an easy-to-mock layer for CLI and future agent use.
- Detect index dimension at init and switch embedding model accordingly.

### 游닌 Inputs & 游닋 Outputs
| Direction | Name | Type | Brief Description |
|-----------|------|------|-------------------|
| 游닌 In | text | str | Search query text |
| 游닌 In | k | int | Number of results to return |
| 游닌 In | chunk_dir | Path (optional) | Directory holding text chunks for retrieval |
| 游닌 In | return_text | bool (optional) | Include chunk text when `chunk_dir` is configured |
| 游닋 Out | results | List[Tuple[str, float]] or List[Tuple[str, float, str]] | Matching IDs and scores, with text when requested |

### 游댕 Dependencies
- `core.embeddings.embedder.embed_text`
- `core.vectorstore.faiss_store.FaissStore`
- `numpy`
- `core.utils.logger`

### 游딖 Dialogic Notes
- Embedding model is configurable; defaults to OpenAI `text-embedding-3-small`.
- Agents will call this layer instead of accessing FAISS directly.
- When no model is specified, the retriever infers one by reading the FAISS index dimension.
- Uses `id_map.json` to translate FAISS integer IDs back to document filenames.
- When embeddings include chunk IDs (`doc_chunk01`), results may refer to those composite identifiers and `return_text` can load the chunk file if available.
